{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss Model Capptu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time \n",
    "import keras.backend as K\n",
    "import keras \n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import mean_absolute_error, categorical_crossentropy,mean_absolute_error\n",
    "from keras.layers import Flatten, Dropout, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Lambda, concatenate, Conv2D, MaxPooling2D\n",
    "from keras.models import Input,Model, Sequential\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "TESTING_SIZE = 2000\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "TOTAL_IMAGES = 180000\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "BATCH_SIZE = 16#8\n",
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 100\n",
    "TRIPLET_INDEX = 0\n",
    "FINISH_TRIPLET_INDEX = BATCH_SIZE*STEPS_PER_EPOCH*2\n",
    "\n",
    "\n",
    "ENCODINGS_DIM = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arod = h5py.File('./AROD_HDF/AROD.hdf','r')\n",
    "triplets = pd.read_csv('./triplets.csv').get_values()[0:TRAINING_SIZE]\n",
    "training_set = triplets[:,1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet():\n",
    "    global TRIPLET_INDEX\n",
    "    triplet = training_set[TRIPLET_INDEX]\n",
    "    \n",
    "    a = arod['IMAGES'][triplet[0]]\n",
    "    p = arod['IMAGES'][triplet[1]]\n",
    "    n = arod['IMAGES'][triplet[2]]\n",
    "    \n",
    "    sa = arod['SCORES'][triplet[0]][0]        \n",
    "    sp = arod['SCORES'][triplet[1]][0]        \n",
    "    sn = arod['SCORES'][triplet[2]][0]        \n",
    "    TRIPLET_INDEX = TRIPLET_INDEX + 1\n",
    "    if TRIPLET_INDEX > FINISH_TRIPLET_INDEX:\n",
    "        TRIPLET_INDEX = 0 \n",
    "    return a, p, n, sa, sp, sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate():\n",
    "    while True:\n",
    "        list_a = []\n",
    "        list_p = []\n",
    "        list_n = []\n",
    "        label = []\n",
    "        la = []\n",
    "        ln = []\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            a, p, n, sa, sp, sn = get_triplet()\n",
    "            list_a.append(a)\n",
    "            list_p.append(p)\n",
    "            list_n.append(n)\n",
    "            la.append([sa])\n",
    "            ln.append([sn])\n",
    "            label.append([sa,sn])\n",
    "            \n",
    "        A = preprocess_input(np.array(list_a, dtype = 'float32'))\n",
    "        B = preprocess_input(np.array(list_p, dtype = 'float32'))\n",
    "        C = preprocess_input(np.array(list_n, dtype = 'float32'))\n",
    "        label = np.array(label,dtype = 'float32')\n",
    "        photos = [A,B,C]\n",
    "        labels = [np.array(la,dtype='float32'),np.array(ln,dtype='float32')]\n",
    "        \n",
    "        \n",
    "        yield photos, labels\n",
    "        \n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIPLET_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "\n",
    "$L_e (a,p,n) = [m_e + ||\\Phi_a - \\Phi_p||^2  - ||\\Phi_a - \\Phi_n||^2 ]$ \n",
    "\n",
    "$L_d (a,p,n) = sign (s(n) - s(a) )  [m_d + ||\\Phi_a|| - ||\\Phi_n|| ]  $ \n",
    "\n",
    "$Loss = L_d + L_e$\n",
    "\n",
    "Where :\n",
    "\n",
    "$\\Phi_i:$ Encodings of $ith$ Image\n",
    "\n",
    "$m_d, m_e:$ Margins to avoid Trivial loss response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_generator = Generate()\n",
    "#batch = next(train_generator)\n",
    "############## LOSS Function ########################### \n",
    "def identity_loss(y_true, y_pred):\n",
    "    r = y_true[0] - y_pred[0]\n",
    "    #return K.mean(y_pred - 0 * y_true)\n",
    "    return K.sum(y_pred - 0 * y_true,axis=-1)\n",
    "\n",
    "def Le(X):\n",
    "    a, p, n = X\n",
    "    m = 0.3 \n",
    "    loss = K.relu(m + K.sum(K.square(a-p),axis=-1,keepdims=True) - K.sum(K.square(a-n),axis=-1,keepdims=True))\n",
    "    return loss\n",
    "\n",
    "def Ld_1(X):\n",
    "    a, p, n = X\n",
    "    m = 0.2\n",
    "    loss = K.relu(m+ K.sqrt(K.sum(K.square(a),axis=-1,keepdims=True)) - K.sqrt(K.sum(K.square(n),axis=-1,keepdims=True)))\n",
    "    return loss\n",
    "\n",
    "def triplet_loss(y_true,y_pred):\n",
    "    sa = y_true[0]\n",
    "    sn = y_true[1]\n",
    "    #sn = y_true[2]\n",
    "    \n",
    "    le = y_pred[0]\n",
    "    ld = y_pred[1]\n",
    "    \n",
    "    return (sn - sa)*ld + le\n",
    "\n",
    "def fake_triplet_loss(y_true,y_pred):\n",
    "    sa = y_true[0]\n",
    "    sn = y_true[1]\n",
    "    ##sn = y_true[2]\n",
    "    le = y_pred[0]\n",
    "    ld = y_pred[1]\n",
    "#     sa,sn = y_true\n",
    "#     le,ld = y_pred\n",
    "    \n",
    "    \n",
    "    return (sn - sa) * ld + le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.random.random((1000,)),np.random.random((1000,)),np.random.random((1000,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.76003736])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(Le(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.760037361103514"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([0,.3 + np.sum(np.square(X[0]-X[1]))-np.sum(np.square(X[0]-X[2]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net model \n",
    "<img src=\"./Capptu model Resnet50_tripletloss.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1000)         84392       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 84,392\n",
      "Trainable params: 84,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############## Model ########################### \n",
    "def GetBaseModel():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    dense_1 = Dense(ENCODINGS_DIM,activation='sigmoid')(x)\n",
    "    #normalized = Lambda(lambda  x: K.l2_normalize(x,axis=1))(dense_1)\n",
    "    base_model = Model(base_model.input, dense_1, name=\"base_model\")\n",
    "    return base_model\n",
    "\n",
    "def GetModel(base_model):\n",
    "    input_1 = Input((IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "    input_2 = Input((IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "    input_3 = Input((IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "\n",
    "    r1 = base_model(input_1)\n",
    "    r2 = base_model(input_2)\n",
    "    r3= base_model(input_3)\n",
    "\n",
    "    loss_le = Lambda(Le)([r1,r2,r3])\n",
    "    loss_ld1 = Lambda(Ld_1)([r1,r2,r3])\n",
    "    #loss = concatenate([loss_le,loss_ld1],axis=-1)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[input_1, input_2, input_3], outputs=[loss_le,loss_ld1])    \n",
    "    model.compile(loss=fake_triplet_loss, optimizer=Adam(lr=.1))#0.000003  default\n",
    "    return model\n",
    "\n",
    "def ToyModel():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3) ,kernel_initializer='random_uniform') )\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='random_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "#    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(1000, activation='sigmoid'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam = Adam()\n",
    "    return model\n",
    "    \n",
    "#model = GetModel(GetBaseModel())\n",
    "#$model.summary()\n",
    "\n",
    "model = GetModel(ToyModel())\n",
    "model.summary()\n",
    "plot_model(to_file='./Capptu model Resnet50_tripletloss.png',model=model,show_layer_names=True,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#filepath=\"weights.best.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')  #   \n",
    "#tb = TensorBoard(log_dir='./logs/', write_graph=True)\n",
    "tb = TensorBoard(log_dir='./log/', histogram_freq=5, batch_size=BATCH_SIZE, write_graph=True, write_grads=True, write_images=True) \n",
    "callbacks_list =[tb]#[] [checkpoint, tb]\n",
    "history = model.fit_generator(Generate(),\n",
    "                    epochs=EPOCHS,#100\n",
    "                    verbose=1, \n",
    "                    workers=20,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,#5000\n",
    "                              callbacks = callbacks_list\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "176/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedmodel = model.layers[3]\n",
    "trainedmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [A,P,N], labels = Generate().next()\n",
    "TRIPLET_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Training time \n",
    "        \n",
    "        for 1000 steps\n",
    "        \n",
    "            batch_size: 8\n",
    "            epochs: 1\n",
    "            triplet index : 8080\n",
    "            time 355s /5.9 min\n",
    "            total images processed : 24240\n",
    "        \n",
    "        \n",
    "        \n",
    "        extrapolating to 100 000 steps\n",
    "        \n",
    "            batch_size: 8\n",
    "            epochs: 1\n",
    "            triplet index : 808000\n",
    "            time 35500s /9.86 min\n",
    "            total images processed : 24240000\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model.save(filepath='first test tripletNN.h5py')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save('second_save.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
