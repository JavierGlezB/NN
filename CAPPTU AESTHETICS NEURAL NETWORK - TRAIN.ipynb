{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss Model Capptu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time \n",
    "import keras.backend as K\n",
    "import keras \n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import mean_absolute_error, categorical_crossentropy,mean_absolute_error\n",
    "from keras.layers import Flatten, Dropout, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Lambda, concatenate, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Input,Model, Sequential\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "TESTING_SIZE = 2000\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "TOTAL_IMAGES = 180000\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "BATCH_SIZE = 16#8\n",
    "EPOCHS = 1\n",
    "STEPS_PER_EPOCH = 1000\n",
    "TRIPLET_INDEX = 0\n",
    "FINISH_TRIPLET_INDEX = BATCH_SIZE*STEPS_PER_EPOCH*2\n",
    "\n",
    "\n",
    "ENCODINGS_DIM = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "arod = h5py.File('./AROD_HDF/AROD.hdf','r')\n",
    "triplets = pd.read_csv('./triplets.csv').get_values()[0:TRAINING_SIZE]\n",
    "training_set = triplets[:,1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet():\n",
    "    global TRIPLET_INDEX\n",
    "    triplet = training_set[TRIPLET_INDEX]\n",
    "    \n",
    "    a = arod['IMAGES'][triplet[0]]\n",
    "    p = arod['IMAGES'][triplet[1]]\n",
    "    n = arod['IMAGES'][triplet[2]]\n",
    "    \n",
    "    sa = arod['SCORES'][triplet[0]][0]        \n",
    "    sp = arod['SCORES'][triplet[1]][0]        \n",
    "    sn = arod['SCORES'][triplet[2]][0]        \n",
    "  \n",
    "    TRIPLET_INDEX = TRIPLET_INDEX + 1\n",
    "    if TRIPLET_INDEX > FINISH_TRIPLET_INDEX:\n",
    "        TRIPLET_INDEX = 0 \n",
    "    return a, p, n, sa, sp, sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate():\n",
    "    while True:\n",
    "        list_a = []\n",
    "        list_p = []\n",
    "        list_n = []\n",
    "        label = []\n",
    "        la = []\n",
    "        lp=[]\n",
    "        ln = []\n",
    "        \n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            a, p, n, sa, sp, sn = get_triplet()\n",
    "            list_a.append(a)\n",
    "            list_p.append(p)\n",
    "            list_n.append(n)\n",
    "            la.append([sa])\n",
    "            ln.append([sn])\n",
    "            lp.append([sp])\n",
    "            label.append([sa,sn])\n",
    "        \n",
    "        \n",
    "        zeromat = np.zeros((BATCH_SIZE,ENCODINGS_DIM-1))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        A = preprocess_input(np.array(list_a, dtype = 'float32'))\n",
    "        B = preprocess_input(np.array(list_p, dtype = 'float32'))\n",
    "        C = preprocess_input(np.array(list_n, dtype = 'float32'))\n",
    "        label = np.array(label,dtype = 'float32')\n",
    "        photos = [A,B,C]\n",
    "        labels = [np.append(np.array(la),zeromat,axis=-1),np.append(np.array(lp),zeromat,axis=-1),np.append(np.array(ln),zeromat,axis=-1)]#[np.array(la,dtype='float32'),np.array(ln,dtype='float32')]\n",
    "        yield photos, labels\n",
    "        \n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "[A,P,N], labels = Generate().next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40476879, 0.57713735, 0.17069401, 0.36863664, 0.32091293,\n",
       "       0.45853236, 0.31629398, 0.55512029, 0.13033688, 0.16054408,\n",
       "       0.46841747, 0.35895741, 0.34056053, 0.5638805 , 0.62254858,\n",
       "       0.68500602])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIPLET_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "\n",
    "$L_e (a,p,n) = [m_e + ||\\Phi_a - \\Phi_p||^2  - ||\\Phi_a - \\Phi_n||^2 ]$ \n",
    "\n",
    "$L_d (a,p,n) = sign (s(n) - s(a) )  [m_d + ||\\Phi_a|| - ||\\Phi_n|| ]  $ \n",
    "\n",
    "$Loss = L_d + L_e$\n",
    "\n",
    "Where :\n",
    "\n",
    "$\\Phi_i:$ Encodings of $ith$ Image\n",
    "\n",
    "$m_d, m_e:$ Margins to avoid Trivial loss response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_generator = Generate()\n",
    "#batch = next(train_generator)\n",
    "############## LOSS Function ########################### \n",
    "def identity_loss(y_true, y_pred):\n",
    "    r = y_true[0] - y_pred[0]\n",
    "    #return K.mean(y_pred - 0 * y_true)\n",
    "    return K.sum(y_pred - 0 * y_true,axis=-1)\n",
    "\n",
    "def Le(X):\n",
    "    a, p, n = X\n",
    "    m = 0.3 \n",
    "    loss = K.relu(m + K.sum(K.square(a-p),axis=-1,keepdims=True) - K.sum(K.square(a-n),axis=-1,keepdims=True))\n",
    "    return loss\n",
    "\n",
    "def Ld_1(X):\n",
    "    a, p, n = X\n",
    "    m = 0.2\n",
    "    loss = K.relu(m+ K.sqrt(K.sum(K.square(a),axis=-1,keepdims=True)) - K.sqrt(K.sum(K.square(n),axis=-1,keepdims=True)))\n",
    "    return loss\n",
    "\n",
    "def triplet_loss(y_true,y_pred):\n",
    "    sa = y_true[0]\n",
    "    sn = y_true[1]\n",
    "    #sn = y_true[2]\n",
    "    \n",
    "    le = y_pred[0]\n",
    "    ld = y_pred[1]\n",
    "    \n",
    "    return (sn - sa)*ld + le\n",
    "\n",
    "def fake_triplet_loss(y_true,y_pred):\n",
    "    sa = y_true[0]\n",
    "    sn = y_true[1]\n",
    "    ##sn = y_true[2]\n",
    "    le = y_pred[0]\n",
    "    ld = y_pred[1]\n",
    "#     sa,sn = y_true\n",
    "#     le,ld = y_pred\n",
    "    \n",
    "    \n",
    "    return (sn - sa) * ld + le\n",
    "\n",
    "def newTripletLoss (y_true,y_pred):\n",
    "    #sa = #K.mean(y_true[0],axis=-1)\n",
    "    #sp = #K.mean(y_true[1],axis=-1)\n",
    "    #sn = #K.mean(y_true[2],axis=-1)\n",
    "    \n",
    "    ea = y_true[0]\n",
    "    ep = y_true[1]\n",
    "    en = y_true[2]\n",
    "    \n",
    "    return ea - ep\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.random.random((1000,)),np.random.random((1000,)),np.random.random((1000,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.90037867])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(Le(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.90037866799014"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([0,.3 + np.sum(np.square(X[0]-X[1]))-np.sum(np.square(X[0]-X[2]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net model \n",
    "<img src=\"./Capptu model Resnet50_tripletloss.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_58 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_59 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_60 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_20 (Sequential)      (None, 1000)         84392       input_58[0][0]                   \n",
      "                                                                 input_59[0][0]                   \n",
      "                                                                 input_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 3000)         0           sequential_20[1][0]              \n",
      "                                                                 sequential_20[2][0]              \n",
      "                                                                 sequential_20[3][0]              \n",
      "==================================================================================================\n",
      "Total params: 84,392\n",
      "Trainable params: 84,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############## Model ########################### \n",
    "def BaseModel():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    dense_1 = Dense(ENCODINGS_DIM,activation='sigmoid')(x)\n",
    "    #normalized = Lambda(lambda  x: K.l2_normalize(x,axis=1))(dense_1)\n",
    "    base_model = Model(base_model.input, dense_1, name=\"base_model\")\n",
    "    return base_model\n",
    "\n",
    "def GetModel(base_model):\n",
    "    input_1 = Input((IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "    input_2 = Input((IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "    input_3 = Input((IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "\n",
    "    r1 = base_model(input_1)\n",
    "    r2 = base_model(input_2)\n",
    "    r3= base_model(input_3)\n",
    "\n",
    "    #loss_le = Lambda(Le)([r1,r2,r3])\n",
    "    #loss_ld1 = Lambda(Ld_1)([r1,r2,r3])\n",
    "    loss = concatenate([r1,r2,r3])\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[input_1, input_2, input_3], outputs=[loss])    \n",
    "    model.compile(loss=newTripletLoss, optimizer=Adam(lr=.1))#0.000003  default\n",
    "    return model\n",
    "\n",
    "def ToyModel():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3) ,kernel_initializer='random_uniform') )\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='random_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "#    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(1000, activation='sigmoid'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam = Adam()\n",
    "    return model\n",
    "    \n",
    "#model = GetModel(GetBaseModel())\n",
    "#$model.summary()\n",
    "\n",
    "model = GetModel(ToyModel())\n",
    "model.summary()\n",
    "plot_model(to_file='./Capptu model Resnet50_tripletloss.png',model=model,show_layer_names=True,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "[A,P,N], labels = Generate().next()\n",
    "outs = model.predict([A,P,N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1502.0376, 1501.5044, 1499.3223, 1501.139 , 1499.1427, 1501.2534,\n",
       "       1501.7089, 1505.4724, 1501.8417, 1502.9618, 1500.8629, 1504.9036,\n",
       "       1504.9573, 1502.9055, 1505.7146, 1495.3583], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(identity_loss(outs,outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2052389 , 0.81132483, 0.5026194 , ..., 0.8212099 , 0.52416533,\n",
       "        0.868542  ],\n",
       "       [0.44474813, 0.59270984, 0.3090063 , ..., 0.62385046, 0.51793474,\n",
       "        0.8534364 ],\n",
       "       [0.21053617, 0.7905062 , 0.51991326, ..., 0.7158628 , 0.43994832,\n",
       "        0.8312026 ],\n",
       "       ...,\n",
       "       [0.39033273, 0.57518905, 0.34032845, ..., 0.7245835 , 0.514397  ,\n",
       "        0.83174086],\n",
       "       [0.22263691, 0.6517118 , 0.37178037, ..., 0.82819456, 0.49352893,\n",
       "        0.8978176 ],\n",
       "       [0.37366557, 0.7392899 , 0.34117013, ..., 0.811876  , 0.43593508,\n",
       "        0.86374336]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-9f04cc1be1b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m#5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                               \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         )\n",
      "\u001b[0;32m/home/capptu/envs/TensorFlow/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/capptu/envs/TensorFlow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2080\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2081\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/capptu/envs/TensorFlow/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    990\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    991\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    993\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m                 \u001b[0;31m# Gets loss and metrics. Updates weights at each call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/capptu/envs/TensorFlow/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/capptu/envs/TensorFlow/local/lib/python2.7/site-packages/keras/optimizers.pyc\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/capptu/envs/TensorFlow/local/lib/python2.7/site-packages/keras/optimizers.pyc\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             raise ValueError('An operation has `None` for gradient. '\n\u001b[0m\u001b[1;32m     81\u001b[0m                              \u001b[0;34m'Please make sure that all of your ops have a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                              \u001b[0;34m'gradient defined (i.e. are differentiable). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#filepath=\"weights.best.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')  #   \n",
    "tb = TensorBoard(log_dir='./logs/', write_graph=True)\n",
    "#tb = TensorBoard(log_dir='./log/', histogram_freq=5, write_graph=True, write_grads=True, write_images=True) \n",
    "callbacks_list =[tb]#[] [checkpoint, tb]\n",
    "history = model.fit_generator(Generate(),\n",
    "                    epochs=EPOCHS,#100\n",
    "                    verbose=1, \n",
    "                    workers=20,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,#5000\n",
    "                              callbacks = callbacks_list\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIPLET_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "751/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedmodel = model.layers[3]\n",
    "trainedmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainedmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-b995960ecef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainedmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainedmodel' is not defined"
     ]
    }
   ],
   "source": [
    "[A,P,N], labels = Generate().next()\n",
    "outs = trainedmodel.predict(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Training time \n",
    "        \n",
    "        for 1000 steps\n",
    "        \n",
    "            batch_size: 8\n",
    "            epochs: 1\n",
    "            triplet index : 8080\n",
    "            time 355s /5.9 min\n",
    "            total images processed : 24240\n",
    "        \n",
    "        \n",
    "        \n",
    "        extrapolating to 100 000 steps\n",
    "        \n",
    "            batch_size: 8\n",
    "            epochs: 1\n",
    "            triplet index : 808000\n",
    "            time 35500s /9.86 min\n",
    "            total images processed : 24240000\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model.save(filepath='first test tripletNN.h5py')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save('second_save.h5py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
